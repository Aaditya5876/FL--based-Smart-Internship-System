import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import json
from copy import deepcopy
import pandas as pd
import os
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    """Data preprocessor to handle mixed data types and ensure consistency"""
    
    def __init__(self):
        self.feature_columns = None
        self.target_column = None
        self.label_encoders = {}
        self.scalers = {}
        self.global_feature_dim = 9  # Target dimension
        
    def preprocess_dataframe(self, df, is_first_client=False):
        """Preprocess dataframe to handle mixed data types"""
        df_processed = df.copy()
        
        # Identify numeric and non-numeric columns
        numeric_columns = df_processed.select_dtypes(include=[np.number]).columns.tolist()
        non_numeric_columns = df_processed.select_dtypes(exclude=[np.number]).columns.tolist()
        
        print(f"    ğŸ“Š Found {len(numeric_columns)} numeric and {len(non_numeric_columns)} non-numeric columns")
        
        # Handle non-numeric columns
        for col in non_numeric_columns:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                # Fit only on non-null string values
                non_null_values = df_processed[col].dropna().astype(str)
                if len(non_null_values) > 0:
                    self.label_encoders[col].fit(non_null_values)
            
            # Transform column
            try:
                non_null_mask = df_processed[col].notna()
                if non_null_mask.any():
                    df_processed.loc[non_null_mask, col] = self.label_encoders[col].transform(
                        df_processed.loc[non_null_mask, col].astype(str)
                    )
                # Fill NaN with 0
                df_processed[col] = df_processed[col].fillna(0)
                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(0)
            except Exception as e:
                print(f"      âš ï¸ Warning: Could not encode column '{col}': {e}")
                # Fill with zeros if encoding fails
                df_processed[col] = 0
        
        # Handle numeric columns - fill NaN with median
        for col in numeric_columns:
            if df_processed[col].isna().any():
                median_val = df_processed[col].median()
                df_processed[col] = df_processed[col].fillna(median_val if not pd.isna(median_val) else 0)
        
        # Convert all columns to numeric
        for col in df_processed.columns:
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(0)
        
        return df_processed
    
    def extract_features_and_target(self, df_processed):
        """Extract features and target from processed dataframe"""
        # Use last column as target, rest as features
        if len(df_processed.columns) > 1:
            features = df_processed.iloc[:, :-1].values
            targets = df_processed.iloc[:, -1].values
        else:
            # If only one column, use it as target and create dummy features
            features = np.random.randn(len(df_processed), self.global_feature_dim)
            targets = df_processed.iloc[:, 0].values
            print(f"      âš ï¸ Only 1 column found, using as target with dummy features")
        
        # Ensure features have consistent dimensions
        if features.shape[1] < self.global_feature_dim:
            # Pad with zeros
            padding = np.zeros((features.shape[0], self.global_feature_dim - features.shape[1]))
            features = np.hstack([features, padding])
            print(f"      ğŸ“ Padded features from {features.shape[1]} to {self.global_feature_dim} dimensions")
        elif features.shape[1] > self.global_feature_dim:
            # Truncate
            features = features[:, :self.global_feature_dim]
            print(f"      âœ‚ï¸ Truncated features from {features.shape[1]} to {self.global_feature_dim} dimensions")
        
        # CRITICAL: Scale the data to prevent NaN losses
        from sklearn.preprocessing import StandardScaler, RobustScaler
        
        # Use RobustScaler to handle outliers better than StandardScaler
        feature_scaler = RobustScaler()
        target_scaler = RobustScaler()
        
        # Scale features
        features = feature_scaler.fit_transform(features)
        print(f"      ğŸ”§ Scaled features - Range: [{features.min():.3f}, {features.max():.3f}]")
        
        # Scale targets and clamp extreme values
        targets_reshaped = targets.reshape(-1, 1)
        targets_scaled = target_scaler.fit_transform(targets_reshaped).flatten()
        
        # Clamp targets to reasonable range to prevent exploding gradients
        targets_scaled = np.clip(targets_scaled, -10, 10)
        print(f"      ğŸ”§ Scaled targets - Range: [{targets_scaled.min():.3f}, {targets_scaled.max():.3f}]")
        
        # Store scalers for potential inverse transform
        if not hasattr(self, 'feature_scaler'):
            self.feature_scaler = feature_scaler
            self.target_scaler = target_scaler
        
        return features.astype(np.float32), targets_scaled.astype(np.float32)

class FedProxClient:
    def __init__(self, client_id, model, data, targets, mu=0.1):
        """
        FedProx Client with proximal term
        
        Args:
            client_id: Unique identifier for client
            model: Neural network model
            data: Client's training data
            targets: Client's training targets
            mu: Proximal parameter (0.1 is optimal balance)
        """
        self.client_id = client_id
        self.model = model
        self.data = data
        self.targets = targets
        self.mu = mu  # Proximal parameter
        
        # Split data into train/test
        X_train, X_test, y_train, y_test = train_test_split(
            data, targets, test_size=0.2, random_state=42
        )
        
        self.train_loader = DataLoader(
            TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
            batch_size=32, shuffle=True
        )
        
        self.test_loader = DataLoader(
            TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test)),
            batch_size=32, shuffle=False
        )
        
    def local_train(self, global_model, epochs=5, lr=0.001):  # Reduced learning rate
        """
        FedProx local training with proximal term and gradient clipping
        
        Args:
            global_model: Current global model parameters
            epochs: Number of local training epochs
            lr: Learning rate (reduced default)
        """
        self.model.train()
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)  # Use Adam with weight decay
        criterion = nn.MSELoss()
        
        total_loss = 0
        num_samples = 0
        
        # Store global model parameters for proximal term
        global_params = {}
        for name, param in global_model.named_parameters():
            global_params[name] = param.clone().detach()
        
        for epoch in range(epochs):
            epoch_loss = 0
            epoch_samples = 0
            
            for batch_data, batch_targets in self.train_loader:
                # Check for NaN in input data
                if torch.isnan(batch_data).any() or torch.isnan(batch_targets).any():
                    print(f"âš ï¸ Warning: NaN detected in batch for client {self.client_id}")
                    continue
                
                optimizer.zero_grad()
                
                # Forward pass
                outputs = self.model(batch_data)
                
                # Check for NaN in outputs
                if torch.isnan(outputs).any():
                    print(f"âš ï¸ Warning: NaN in model outputs for client {self.client_id}")
                    continue
                
                # Original loss
                original_loss = criterion(outputs.squeeze(), batch_targets)
                
                # Check for NaN in loss
                if torch.isnan(original_loss):
                    print(f"âš ï¸ Warning: NaN in loss for client {self.client_id}")
                    continue
                
                # FedProx proximal term: Î¼/2 * ||w - w_global||Â²
                proximal_loss = 0
                for name, param in self.model.named_parameters():
                    if name in global_params and not torch.isnan(param).any():  # Safety check
                        proximal_loss += torch.norm(param - global_params[name]) ** 2
                proximal_loss = (self.mu / 2) * proximal_loss
                
                # Total FedProx loss
                total_fedprox_loss = original_loss + proximal_loss
                
                # Check final loss for NaN
                if torch.isnan(total_fedprox_loss):
                    print(f"âš ï¸ Warning: NaN in total loss for client {self.client_id}")
                    continue
                
                # Backward pass
                total_fedprox_loss.backward()
                
                # Gradient clipping to prevent exploding gradients
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += total_fedprox_loss.item() * len(batch_data)
                epoch_samples += len(batch_data)
            
            total_loss += epoch_loss
            num_samples += epoch_samples
        
        avg_loss = total_loss / num_samples if num_samples > 0 else float('nan')
        
        # Final check for NaN loss
        if np.isnan(avg_loss):
            print(f"âŒ Training failed for client {self.client_id}: NaN loss detected")
            
        return avg_loss, num_samples
    
    def evaluate(self):
        """Evaluate model on test data"""
        self.model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_data, batch_targets in self.test_loader:
                outputs = self.model(batch_data)
                predictions.extend(outputs.squeeze().cpu().numpy())
                actuals.extend(batch_targets.cpu().numpy())
        
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        mse = mean_squared_error(actuals, predictions)
        mae = mean_absolute_error(actuals, predictions)
        r2 = r2_score(actuals, predictions)
        
        return {
            'client_id': self.client_id,
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'num_test_samples': len(actuals)
        }

class FedProxServer:
    def __init__(self, model_template, clients, mu=0.1):
        """
        FedProx Server for aggregation
        
        Args:
            model_template: Template model architecture
            clients: List of FedProx clients
            mu: Proximal parameter (shared across all clients)
        """
        self.global_model = deepcopy(model_template)
        self.clients = clients
        self.mu = mu
        
        # Set mu for all clients
        for client in self.clients:
            client.mu = self.mu
    
    def aggregate_models(self, client_models, client_weights):
        """
        FedProx aggregation with improved error handling
        """
        if not client_models:
            print("âš ï¸ Warning: No client models to aggregate")
            return
            
        # Initialize aggregated parameters
        aggregated_params = {}
        
        # Get parameter names from first model
        first_model_params = list(client_models[0].named_parameters())
        
        # Initialize aggregated parameters with zeros
        for name, param in first_model_params:
            aggregated_params[name] = torch.zeros_like(param)
        
        # Weighted aggregation with safety checks
        total_weight = sum(client_weights)
        
        for i, (model, weight) in enumerate(zip(client_models, client_weights)):
            for name, param in model.named_parameters():
                if name in aggregated_params:
                    # Check tensor shapes match
                    if param.shape == aggregated_params[name].shape:
                        aggregated_params[name] += (weight / total_weight) * param.data
                    else:
                        print(f"âš ï¸ Warning: Shape mismatch for parameter '{name}' in client {i}")
                        print(f"    Expected: {aggregated_params[name].shape}, Got: {param.shape}")
        
        # Update global model with safety checks
        global_state_dict = self.global_model.state_dict()
        for name, param in aggregated_params.items():
            if name in global_state_dict:
                if param.shape == global_state_dict[name].shape:
                    global_state_dict[name].copy_(param)
                else:
                    print(f"âš ï¸ Warning: Cannot update global parameter '{name}' due to shape mismatch")
    
    def train_round(self, round_num, local_epochs=3, lr=0.001):  # Reduced epochs and learning rate
        """Execute one round of FedProx training"""
        print(f"=== FedProx Round {round_num} (Î¼={self.mu}) ===")
        
        # Distribute global model to clients
        client_models = []
        client_weights = []
        training_metrics = []
        successful_clients = 0
        
        for client in self.clients:
            try:
                # Copy global model to client
                client.model.load_state_dict(self.global_model.state_dict())
                
                # Local training with proximal term
                loss, num_samples = client.local_train(
                    self.global_model, epochs=local_epochs, lr=lr
                )
                
                # Skip clients with NaN losses
                if np.isnan(loss):
                    print(f"âš ï¸ Skipping client {client.client_id} due to NaN loss")
                    continue
                
                # Collect trained model and metrics
                client_models.append(deepcopy(client.model))
                client_weights.append(num_samples)
                successful_clients += 1
                
                training_metrics.append({
                    'client_id': client.client_id,
                    'final_loss': loss,
                    'num_samples': num_samples
                })
                
                print(f"Client {client.client_id}: Loss={loss:.6f}, Samples={num_samples}")
                
            except Exception as e:
                print(f"âŒ Error training client {client.client_id}: {e}")
                continue
        
        if not client_models:
            print("âŒ No successful client training in this round!")
            return None
        
        print(f"âœ… Successfully trained {successful_clients}/{len(self.clients)} clients")
        
        # Server aggregation
        try:
            self.aggregate_models(client_models, client_weights)
        except Exception as e:
            print(f"âŒ Error in model aggregation: {e}")
            return None
        
        # Evaluation
        eval_metrics = []
        total_mse = 0
        total_mae = 0
        total_r2 = 0
        total_samples = 0
        successful_evaluations = 0
        
        for client in self.clients:
            try:
                # Update client model with aggregated global model
                client.model.load_state_dict(self.global_model.state_dict())
                
                # Evaluate
                metrics = client.evaluate()
                
                # Skip clients with invalid metrics
                if np.isnan(metrics['mse']) or np.isnan(metrics['mae']) or np.isnan(metrics['r2']):
                    print(f"âš ï¸ Skipping evaluation for client {client.client_id} due to NaN metrics")
                    continue
                
                eval_metrics.append(metrics)
                successful_evaluations += 1
                
                # Aggregate metrics
                total_mse += metrics['mse'] * metrics['num_test_samples']
                total_mae += metrics['mae'] * metrics['num_test_samples']
                total_r2 += metrics['r2'] * metrics['num_test_samples']
                total_samples += metrics['num_test_samples']
                
                print(f"Client {client.client_id}: MSE={metrics['mse']:.6f}, MAE={metrics['mae']:.6f}, RÂ²={metrics['r2']:.6f}")
                
            except Exception as e:
                print(f"âŒ Error evaluating client {client.client_id}: {e}")
                continue
        
        # Calculate average metrics
        if total_samples > 0 and successful_evaluations > 0:
            avg_mse = total_mse / total_samples
            avg_mae = total_mae / total_samples
            avg_r2 = total_r2 / total_samples
        else:
            avg_mse = avg_mae = avg_r2 = float('nan')
            print("âš ï¸ Warning: No successful evaluations in this round")
        
        print(f"Round {round_num} Averages: MSE={avg_mse:.6f}, MAE={avg_mae:.6f}, RÂ²={avg_r2:.6f}")
        print(f"Successful clients: {successful_clients}/{len(self.clients)}, Evaluations: {successful_evaluations}")
        print("-" * 60)
        
        return {
            'round': round_num,
            'training_metrics': training_metrics,
            'eval_metrics': eval_metrics,
            'avg_mse': avg_mse if not np.isnan(avg_mse) else 0.0,
            'avg_mae': avg_mae if not np.isnan(avg_mae) else 0.0,
            'avg_r2': avg_r2 if not np.isnan(avg_r2) else 0.0,
            'successful_clients': successful_clients,
            'successful_evaluations': successful_evaluations
        }

class JobRecommendationModel(nn.Module):
    def __init__(self, input_dim=9, hidden_dim=32):  # Reduced hidden dimension
        super(JobRecommendationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)
        self.fc3 = nn.Linear(hidden_dim//2, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)
        self.batch_norm2 = nn.BatchNorm1d(hidden_dim//2)
        
        # Initialize weights properly
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize weights using Xavier initialization"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.batch_norm1(x) if x.size(0) > 1 else x  # Skip batch norm for single samples
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc2(x)
        x = self.batch_norm2(x) if x.size(0) > 1 else x  # Skip batch norm for single samples
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc3(x)
        return x

# ... (keep all the imports and class definitions the same) ...

def run_fedprox_experiment(client_data, num_rounds=10, mu=0.1, experiment_name="fedprox_experiment"):
    """
    Run complete FedProx experiment with improved error handling
    
    Args:
        client_data: Dictionary with client data
        num_rounds: Number of federated rounds
        mu: Proximal parameter (0.1 recommended)
        experiment_name: Name for this experiment run
    """
    
    print(f"ğŸš€ Starting FedProx Experiment with Î¼={mu}")
    print(f"Clients: {len(client_data)}, Rounds: {num_rounds}")
    print("=" * 70)
    
    if not client_data:
        print("âŒ No client data available!")
        return None, None
    
    # Create model template
    model_template = JobRecommendationModel(input_dim=9)
    
    # Create FedProx clients
    clients = []
    for client_id, data in client_data.items():
        try:
            client_model = JobRecommendationModel(input_dim=9)
            client = FedProxClient(
                client_id=client_id,
                model=client_model,
                data=data['features'],
                targets=data['targets'],
                mu=mu
            )
            clients.append(client)
            print(f"âœ… Created FedProx client: {client_id} ({len(data['features'])} samples)")
        except Exception as e:
            print(f"âŒ Error creating client {client_id}: {e}")
            continue
    
    if not clients:
        print("âŒ No clients created successfully!")
        return None, None
    
    # Create FedProx server
    server = FedProxServer(model_template, clients, mu=mu)
    
    # Training loop
    results = []
    
    for round_num in range(1, num_rounds + 1):
        round_result = server.train_round(round_num)
        if round_result is not None:
            results.append(round_result)
        else:
            print(f"âš ï¸ Skipping round {round_num} due to errors")
    
    if not results:
        print("âŒ No successful training rounds!")
        return None, None
    
    # Final results
    final_result = results[-1]
    
    print("\n" + "=" * 70)
    print("ğŸ¯ FEDPROX FINAL RESULTS")
    print("=" * 70)
    print(f"Proximal Parameter (Î¼): {mu}")
    print(f"Final Average MSE: {final_result['avg_mse']:.6f}")
    print(f"Final Average MAE: {final_result['avg_mae']:.6f}")
    print(f"Final Average RÂ²: {final_result['avg_r2']:.6f}")
    print("=" * 70)
    
    # Comparison with FedAvg baseline
    fedavg_mse = 0.026497546690611083  # Your baseline result
    fedavg_mae = 0.12492053470565838
    fedavg_r2 = -0.01211075296176678
    
    mse_improvement = ((fedavg_mse - final_result['avg_mse']) / fedavg_mse) * 100
    mae_improvement = ((fedavg_mae - final_result['avg_mae']) / fedavg_mae) * 100
    r2_improvement = ((final_result['avg_r2'] - fedavg_r2) / abs(fedavg_r2)) * 100
    
    print("ğŸ“Š COMPARISON WITH FEDAVG BASELINE:")
    print(f"MSE Improvement: {mse_improvement:.2f}% {'âœ… Better' if mse_improvement > 0 else 'âŒ Worse'}")
    print(f"MAE Improvement: {mae_improvement:.2f}% {'âœ… Better' if mae_improvement > 0 else 'âŒ Worse'}")
    print(f"RÂ² Improvement: {r2_improvement:.2f}% {'âœ… Better' if r2_improvement > 0 else 'âŒ Worse'}")
    
    # Create comprehensive results dictionary
    results_dict = {
        'algorithm': 'FedProx',
        'mu': mu,
        'rounds': num_rounds,
        'final_performance': final_result,
        'all_rounds': results,
        'improvements': {
            'mse_improvement_pct': mse_improvement,
            'mae_improvement_pct': mae_improvement,
            'r2_improvement_pct': r2_improvement
        }
    }
    
    return results_dict, final_result['avg_mse']

# MAIN EXECUTION (modified for tuning)
if __name__ == "__main__":
    # Load your actual data
    client_data = load_your_actual_data()
    
    if client_data:
        # Run FedProx experiment with default mu=0.1
        print("ğŸš€ Starting FedProx Experiment...")
        results, final_mse = run_fedprox_experiment(client_data, num_rounds=10, mu=0.1)
        
        if results:
            # Save results
            try:
                with open('fedprox_results.json', 'w') as f:
                    json.dump(results, f, indent=2, default=str)
                print("âœ… Results saved to 'fedprox_results.json'")
            except Exception as e:
                print(f"âš ï¸ Could not save results to file: {e}")
        else:
            print("âŒ Experiment failed!")
    else:
        print("âŒ No data available to run experiment!")

def load_your_actual_data():
    """
    Load data from your shared/test-data/processed folder structure with enhanced preprocessing
    """
    
    # Base path to your data
    base_path = "../shared/test-data/processed"
    
    # Client folder mapping
    client_folders = {
        'and Sons': 'university_client_and Sons',
        'Group': 'university_client_Group', 
        'Inc': 'university_client_Inc',
        'LLC': 'university_client_LLC',
        'Ltd': 'university_client_Ltd',
        'PLC': 'university_client_PLC'
    }
    
    client_data = {}
    preprocessor = DataPreprocessor()
    
    print("ğŸ”„ Loading client data from processed folders...")
    
    for client_name, folder_name in client_folders.items():
        folder_path = os.path.join(base_path, folder_name)
        
        if not os.path.exists(folder_path):
            print(f"âŒ Folder not found: {folder_path}")
            continue
        
        # Look for CSV files in the client folder
        try:
            csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
        except Exception as e:
            print(f"âŒ Error accessing folder {folder_path}: {e}")
            continue
        
        if not csv_files:
            print(f"âŒ No CSV files found in {folder_path}")
            continue
            
        # Load the first CSV file (assuming one file per client)
        csv_file = csv_files[0]
        file_path = os.path.join(folder_path, csv_file)
        
        try:
            # Load data
            df = pd.read_csv(file_path)
            print(f"âœ… Loaded {client_name}: {df.shape[0]} samples, {df.shape[1]} columns")
            
            # Preprocess the dataframe
            print(f"    ğŸ”„ Preprocessing {client_name}...")
            df_processed = preprocessor.preprocess_dataframe(df)
            
            # Extract features and targets
            features, targets = preprocessor.extract_features_and_target(df_processed)
            
            # Validate data
            if len(features) == 0 or len(targets) == 0:
                print(f"âŒ Error: Empty data after preprocessing for {client_name}")
                continue
            
            # Check for any remaining invalid values
            if np.any(np.isnan(features)) or np.any(np.isnan(targets)):
                print(f"    ğŸ§¹ Cleaning remaining NaN values in {client_name}")
                features = np.nan_to_num(features, nan=0.0)
                targets = np.nan_to_num(targets, nan=0.0)
            
            client_data[client_name] = {
                'features': features,
                'targets': targets
            }
            
            print(f"    ğŸ“Š Final shape - Features: {features.shape}, Targets: {targets.shape}")
            print(f"    âœ… Successfully processed {client_name}")
            
        except Exception as e:
            print(f"âŒ Error loading {client_name}: {e}")
            print(f"    Attempting to load with different encoding...")
            
            # Try with different encoding
            try:
                df = pd.read_csv(file_path, encoding='latin1')
                print(f"    âœ… Loaded {client_name} with latin1 encoding: {df.shape[0]} samples, {df.shape[1]} columns")
                
                # Preprocess
                df_processed = preprocessor.preprocess_dataframe(df)
                features, targets = preprocessor.extract_features_and_target(df_processed)
                
                if np.any(np.isnan(features)) or np.any(np.isnan(targets)):
                    features = np.nan_to_num(features, nan=0.0)
                    targets = np.nan_to_num(targets, nan=0.0)
                
                client_data[client_name] = {
                    'features': features,
                    'targets': targets
                }
                
                print(f"    ğŸ“Š Final shape - Features: {features.shape}, Targets: {targets.shape}")
                print(f"    âœ… Successfully processed {client_name}")
                
            except Exception as e2:
                print(f"âŒ Failed to load {client_name} with any encoding: {e2}")
                continue
    
    if not client_data:
        print("âŒ No data loaded! Using synthetic data for testing...")
        # Fallback to synthetic data if loading fails
        client_data = {
            'and Sons': {
                'features': np.random.randn(947, 9).astype(np.float32),
                'targets': np.random.randn(947).astype(np.float32)
            },
            'Group': {
                'features': np.random.randn(879, 9).astype(np.float32),
                'targets': np.random.randn(879).astype(np.float32)
            },
            'Inc': {
                'features': np.random.randn(720, 9).astype(np.float32),
                'targets': np.random.randn(720).astype(np.float32)
            },
            'LLC': {
                'features': np.random.randn(800, 9).astype(np.float32),
                'targets': np.random.randn(800).astype(np.float32)
            },
            'Ltd': {
                'features': np.random.randn(817, 9).astype(np.float32),
                'targets': np.random.randn(817).astype(np.float32)
            },
            'PLC': {
                'features': np.random.randn(837, 9).astype(np.float32),
                'targets': np.random.randn(837).astype(np.float32)
            }
        }
    
    print(f"ğŸ¯ Successfully loaded data for {len(client_data)} clients")
    return client_data

# MAIN EXECUTION
# MAIN EXECUTION (modified for tuning)
if __name__ == "__main__":
    # Load your actual data
    client_data = load_your_actual_data()
    
    if client_data:
        # Run FedProx experiment with default mu=0.1
        print("ğŸš€ Starting FedProx Experiment...")
        results, final_mse = run_fedprox_experiment(client_data, num_rounds=10, mu=0.1)
        
        if results:
            # Save results
            try:
                with open('fedprox_results.json', 'w') as f:
                    json.dump(results, f, indent=2, default=str)
                print("âœ… Results saved to 'fedprox_results.json'")
            except Exception as e:
                print(f"âš ï¸ Could not save results to file: {e}")
        else:
            print("âŒ Experiment failed!")
    else:
        print("âŒ No data available to run experiment!")